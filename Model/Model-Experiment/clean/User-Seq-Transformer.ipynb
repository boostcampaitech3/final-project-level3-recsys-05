{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "\n",
    "DATA_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Experiment/data'\n",
    "MODEL_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Experiment/model'\n",
    "VAL_TO_IDX_DATA_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Server/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, 'clean_score.csv'))\n",
    "df['problem_id'] = df['problem_id'].astype(int).astype(str)\n",
    "df['target'] = df['target'].astype(int).astype(str)\n",
    "df['key'] = df['user_name'] + '-' + df['target']\n",
    "\n",
    "new_df = []\n",
    "\n",
    "group_df = df.groupby('key')\n",
    "\n",
    "for key, g_df in tqdm(group_df):\n",
    "    new_df.append(g_df[~(g_df.duplicated('problem_id'))].copy())\n",
    "\n",
    "new_df = pd.concat(new_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1486949"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'clean_problem_id_to_idx.json'), 'r', encoding = 'utf-8') as f:\n",
    "    problemId_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'clean_idx_to_problem_id.json'), 'r', encoding = 'utf-8') as f:\n",
    "    idx_to_problemId = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problemId2idx(x):\n",
    "    if x in problemId_to_idx: return int(problemId_to_idx[x]) + 1\n",
    "    else: return np.nan\n",
    "    \n",
    "def get_target2idx(x):\n",
    "    if x in problemId_to_idx: return int(problemId_to_idx[x])\n",
    "    else: return np.nan\n",
    "    \n",
    "new_df['problemId2idx'] = new_df['problem_id'].apply(lambda x : get_problemId2idx(x))\n",
    "new_df = new_df[~(new_df['problemId2idx'].isna())].reset_index(drop = True)\n",
    "new_df['problemId2idx'] = new_df['problemId2idx'].astype(int)\n",
    "\n",
    "new_df['target2idx'] = new_df['target'].apply(lambda x : get_target2idx(x))\n",
    "new_df = new_df[~(new_df['target2idx'].isna())].reset_index(drop = True)\n",
    "new_df['target2idx'] = new_df['target2idx'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1486949"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6866/6866 [00:08<00:00, 844.28it/s] \n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "valid_df = []\n",
    "\n",
    "group_df = new_df.groupby('target')\n",
    "for target, g_df in tqdm(group_df):\n",
    "    gg_df = g_df.groupby('key')\n",
    "    for idx, (userID, ggg_df) in enumerate(gg_df):\n",
    "        if idx == 0:\n",
    "            valid_df.append(ggg_df)\n",
    "        else:\n",
    "            train_df.append(ggg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_df).reset_index(drop = True)\n",
    "valid_df = pd.concat(valid_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'clean_train_df.csv'), index = False)\n",
    "valid_df.to_csv(os.path.join(DATA_PATH, 'clean_valid_df.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_assessmentItemID,\n",
    "        hidden_units,\n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        self.assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, hidden_units, padding_idx = 0) # 문항에 대한 정보\n",
    "\n",
    "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "        \n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units, num_assessmentItemID)\n",
    "        )\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        assessmentItem : (batch_size, max_len)\n",
    "        \"\"\"\n",
    "        assessmentItem = input['assessmentItem']\n",
    "\n",
    "        # masking\n",
    "        mask_pad = torch.BoolTensor(assessmentItem > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, assessmentItem.size(1), assessmentItem.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "\n",
    "        assessmentItem_emb = self.assessmentItemID_emb(assessmentItem.to(self.device))\n",
    "        for block in self.blocks:\n",
    "            assessmentItem_emb, attn_dist = block(assessmentItem_emb, mask)\n",
    "\n",
    "        assessmentItem_emb, _ = self.lstm(assessmentItem_emb)\n",
    "        \n",
    "        output = self.predict_layer(assessmentItem_emb[:, -1])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "\n",
    "        group_df = df.groupby('target')\n",
    "\n",
    "        for target2idx, g_df in group_df:\n",
    "            gg_df = g_df.groupby('key')\n",
    "            for userID, ggg_df in gg_df:\n",
    "                if len(ggg_df) >= 2:\n",
    "                    self.features.append(ggg_df['problemId2idx'].tolist()[:-1])\n",
    "                    self.targets.append(ggg_df['problemId2idx'].tolist()[-1] - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return {\n",
    "            'assessmentItem' : np.array(feature), \n",
    "            'target' : target,\n",
    "            }\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "\n",
    "def make_collate_fn(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len = sample['assessmentItem'].shape[0]\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for sample in samples:\n",
    "        features += [pad_sequence(sample['assessmentItem'], max_len = max_len, padding_value = 0)]\n",
    "        targets += [sample['target']]\n",
    "\n",
    "    return {\n",
    "        'assessmentItem' : torch.tensor(features, dtype = torch.long), \n",
    "        'target': torch.tensor(targets, dtype = torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(len(true_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit\n",
    "\n",
    "def get_rank(pred_list, true_list):\n",
    "    ret_rank = len(pred_list) + 1\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            ret_rank = rank + 1\n",
    "    return 0 if ret_rank == (len(pred_list) + 1) else ret_rank\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for input in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, input['target'].to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader, topk = 10):\n",
    "    model.eval()\n",
    "\n",
    "    hit = 0\n",
    "    ndcg = 0\n",
    "    rank = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            \n",
    "            output = -model(input)\n",
    "            predictions = output.argsort()\n",
    "\n",
    "            targets = input['target']\n",
    "\n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                target = [target.item()]\n",
    "                prediction = prediction.cpu().tolist()[:topk]\n",
    "\n",
    "                hit += get_hit(prediction, target)\n",
    "                ndcg += get_ndcg(prediction, target)\n",
    "                rank += get_rank(prediction, target)\n",
    "    \n",
    "    rank = rank / hit\n",
    "    hit = hit / len(data_loader.dataset)\n",
    "    ndcg = ndcg / len(data_loader.dataset)\n",
    "\n",
    "    return hit, ndcg, rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'clean_problem_id_to_idx.json'), 'r', encoding = 'utf-8') as f:\n",
    "    problemId_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'clean_idx_to_problem_id.json'), 'r', encoding = 'utf-8') as f:\n",
    "    idx_to_problemId = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "hidden_units = 128\n",
    "num_heads = 8\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "num_assessmentItemID = len(problemId_to_idx)\n",
    "\n",
    "model_name = 'clean-User-Seq-Transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'clean_train_df.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'clean_valid_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df = train_df)\n",
    "train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = make_collate_fn,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(df = valid_df)\n",
    "valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = make_collate_fn,\n",
    "        num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASRec(\n",
    "        num_assessmentItemID = num_assessmentItemID,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 8.80483| NDCG@10: 0.02161| HIT@10: 0.02980| Rank@10: 2.70936: 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
      "Epoch:   2| Train loss: 8.38384| NDCG@10: 0.05263| HIT@10: 0.07735| Rank@10: 3.21252: 100%|██████████| 1/1 [00:08<00:00,  8.86s/it]\n",
      "Epoch:   3| Train loss: 7.64891| NDCG@10: 0.07629| HIT@10: 0.10818| Rank@10: 2.90366: 100%|██████████| 1/1 [00:08<00:00,  9.00s/it]\n",
      "Epoch:   4| Train loss: 7.04037| NDCG@10: 0.09601| HIT@10: 0.13239| Rank@10: 2.82483: 100%|██████████| 1/1 [00:08<00:00,  8.87s/it]\n",
      "Epoch:   5| Train loss: 6.55012| NDCG@10: 0.10734| HIT@10: 0.14634| Rank@10: 2.74423: 100%|██████████| 1/1 [00:08<00:00,  8.82s/it]\n",
      "Epoch:   6| Train loss: 6.13076| NDCG@10: 0.11711| HIT@10: 0.15676| Rank@10: 2.66105: 100%|██████████| 1/1 [00:08<00:00,  8.97s/it]\n",
      "Epoch:   7| Train loss: 5.76179| NDCG@10: 0.12300| HIT@10: 0.16527| Rank@10: 2.62522: 100%|██████████| 1/1 [00:08<00:00,  8.56s/it]\n",
      "Epoch:   8| Train loss: 5.42871| NDCG@10: 0.12808| HIT@10: 0.17070| Rank@10: 2.55288: 100%|██████████| 1/1 [00:08<00:00,  8.74s/it]\n",
      "Epoch:   9| Train loss: 5.12883| NDCG@10: 0.13148| HIT@10: 0.17305| Rank@10: 2.48855: 100%|██████████| 1/1 [00:08<00:00,  8.14s/it]\n",
      "Epoch:  10| Train loss: 4.84770| NDCG@10: 0.13392| HIT@10: 0.17628| Rank@10: 2.51374: 100%|██████████| 1/1 [00:08<00:00,  8.53s/it]\n",
      "Epoch:  11| Train loss: 4.59264| NDCG@10: 0.13655| HIT@10: 0.17878| Rank@10: 2.46880: 100%|██████████| 1/1 [00:08<00:00,  8.65s/it]\n",
      "Epoch:  12| Train loss: 4.34526| NDCG@10: 0.13807| HIT@10: 0.18098| Rank@10: 2.51825: 100%|██████████| 1/1 [00:08<00:00,  8.40s/it]\n",
      "Epoch:  13| Train loss: 4.12385| NDCG@10: 0.13848| HIT@10: 0.18200| Rank@10: 2.49435: 100%|██████████| 1/1 [00:08<00:00,  8.64s/it]\n",
      "Epoch:  14| Train loss: 3.90947| NDCG@10: 0.13890| HIT@10: 0.18333| Rank@10: 2.51882: 100%|██████████| 1/1 [00:08<00:00,  8.34s/it]\n",
      "Epoch:  15| Train loss: 3.71093| NDCG@10: 0.14016| HIT@10: 0.18391| Rank@10: 2.50998: 100%|██████████| 1/1 [00:08<00:00,  8.50s/it]\n",
      "Epoch:  16| Train loss: 3.51688| NDCG@10: 0.13919| HIT@10: 0.18068| Rank@10: 2.40455: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n",
      "Epoch:  17| Train loss: 3.33867| NDCG@10: 0.13937| HIT@10: 0.18259| Rank@10: 2.48071: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n",
      "Epoch:  18| Train loss: 3.16768| NDCG@10: 0.13798| HIT@10: 0.18156| Rank@10: 2.51172: 100%|██████████| 1/1 [00:08<00:00,  8.53s/it]\n",
      "Epoch:  19| Train loss: 3.00583| NDCG@10: 0.13811| HIT@10: 0.18171| Rank@10: 2.49758: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n",
      "Epoch:  20| Train loss: 2.85065| NDCG@10: 0.13757| HIT@10: 0.18010| Rank@10: 2.50856: 100%|██████████| 1/1 [00:08<00:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST | Epoch:  15| Train loss: 3.71093| NDCG@10: 0.14016| HIT@10: 0.18391| Rank@10: 2.50998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_ndcg = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            hit, ndcg, rank = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_ndcg < ndcg:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_ndcg = ndcg\n",
    "                best_hit = hit\n",
    "                best_rank = rank\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, model_name + '.pt'))\n",
    "\n",
    "            tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}| Rank@10: {rank:.5f}')\n",
    "    \n",
    "print(f'BEST | Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| NDCG@10: {best_ndcg:.5f}| HIT@10: {best_hit:.5f}| Rank@10: {best_rank:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch:   1| Train loss: 8.80483| NDCG@10: 0.02161| HIT@10: 0.02980| Rank@10: 2.70936: 100%|██████████| 1/1 [00:08<00:00,  8.94s/it]\n",
    "Epoch:   2| Train loss: 8.38381| NDCG@10: 0.05243| HIT@10: 0.07706| Rank@10: 3.19619: 100%|██████████| 1/1 [00:08<00:00,  8.41s/it]\n",
    "Epoch:   3| Train loss: 7.64885| NDCG@10: 0.07625| HIT@10: 0.10818| Rank@10: 2.90366: 100%|██████████| 1/1 [00:08<00:00,  8.38s/it]\n",
    "Epoch:   4| Train loss: 7.04034| NDCG@10: 0.09540| HIT@10: 0.13093| Rank@10: 2.74103: 100%|██████████| 1/1 [00:08<00:00,  8.34s/it]\n",
    "Epoch:   5| Train loss: 6.55008| NDCG@10: 0.10707| HIT@10: 0.14458| Rank@10: 2.65178: 100%|██████████| 1/1 [00:08<00:00,  8.42s/it]\n",
    "Epoch:   6| Train loss: 6.13037| NDCG@10: 0.11682| HIT@10: 0.15632| Rank@10: 2.62723: 100%|██████████| 1/1 [00:08<00:00,  8.46s/it]\n",
    "Epoch:   7| Train loss: 5.76239| NDCG@10: 0.12359| HIT@10: 0.16513| Rank@10: 2.62222: 100%|██████████| 1/1 [00:08<00:00,  8.33s/it]\n",
    "Epoch:   8| Train loss: 5.42821| NDCG@10: 0.12860| HIT@10: 0.17129| Rank@10: 2.60583: 100%|██████████| 1/1 [00:08<00:00,  8.55s/it]\n",
    "Epoch:   9| Train loss: 5.12817| NDCG@10: 0.12910| HIT@10: 0.17158| Rank@10: 2.50128: 100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n",
    "Epoch:  10| Train loss: 4.84865| NDCG@10: 0.13466| HIT@10: 0.17907| Rank@10: 2.59590: 100%|██████████| 1/1 [00:08<00:00,  8.48s/it]\n",
    "Epoch:  11| Train loss: 4.59148| NDCG@10: 0.13501| HIT@10: 0.17716| Rank@10: 2.47142: 100%|██████████| 1/1 [00:08<00:00,  8.45s/it]\n",
    "Epoch:  12| Train loss: 4.34668| NDCG@10: 0.13762| HIT@10: 0.18186| Rank@10: 2.48749: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it]\n",
    "Epoch:  13| Train loss: 4.12242| NDCG@10: 0.13800| HIT@10: 0.18200| Rank@10: 2.53952: 100%|██████████| 1/1 [00:08<00:00,  8.52s/it]\n",
    "Epoch:  14| Train loss: 3.90938| NDCG@10: 0.13800| HIT@10: 0.18303| Rank@10: 2.57899: 100%|██████████| 1/1 [00:08<00:00,  8.68s/it]\n",
    "Epoch:  15| Train loss: 3.70974| NDCG@10: 0.13831| HIT@10: 0.18245| Rank@10: 2.50603: 100%|██████████| 1/1 [00:08<00:00,  8.50s/it]\n",
    "Epoch:  16| Train loss: 3.51845| NDCG@10: 0.13719| HIT@10: 0.18010| Rank@10: 2.48900: 100%|██████████| 1/1 [00:08<00:00,  8.29s/it]\n",
    "Epoch:  17| Train loss: 3.33878| NDCG@10: 0.13784| HIT@10: 0.17980| Rank@10: 2.43837: 100%|██████████| 1/1 [00:08<00:00,  8.27s/it]\n",
    "Epoch:  18| Train loss: 3.16704| NDCG@10: 0.13596| HIT@10: 0.17834| Rank@10: 2.50123: 100%|██████████| 1/1 [00:08<00:00,  8.48s/it]\n",
    "Epoch:  19| Train loss: 3.00685| NDCG@10: 0.13749| HIT@10: 0.18112| Rank@10: 2.54862: 100%|██████████| 1/1 [00:08<00:00,  8.28s/it]\n",
    "Epoch:  20| Train loss: 2.85094| NDCG@10: 0.13580| HIT@10: 0.17848| Rank@10: 2.54030: 100%|██████████| 1/1 [00:08<00:00,  8.70s/it]\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
