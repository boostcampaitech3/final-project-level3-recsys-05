{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "\n",
    "DATA_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Experiment/data'\n",
    "MODEL_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Experiment/model'\n",
    "VAL_TO_IDX_DATA_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Server/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213030/213030 [04:04<00:00, 872.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, 'score.csv'))\n",
    "df = df[~(df['problem_id'].isna())].reset_index(drop = True)\n",
    "df['problem_id'] = df['problem_id'].astype(int).astype(str)\n",
    "df['target'] = df['target'].astype(int).astype(str)\n",
    "df['key'] = df['user_name'] + '-' + df['target']\n",
    "\n",
    "new_df = []\n",
    "\n",
    "group_df = df.groupby('key')\n",
    "\n",
    "for key, g_df in tqdm(group_df):\n",
    "    new_df.append(g_df[~(g_df.duplicated('problem_id'))].copy())\n",
    "\n",
    "new_df = pd.concat(new_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'problemId_to_idx.json'), 'r', encoding = 'utf-8') as f:\n",
    "    problemId_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'idx_to_problemId.json'), 'r', encoding = 'utf-8') as f:\n",
    "    idx_to_problemId = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problemId2idx(x):\n",
    "    if x in problemId_to_idx: return problemId_to_idx[x] + 1\n",
    "    else: return np.nan\n",
    "    \n",
    "def get_target2idx(x):\n",
    "    if x in problemId_to_idx: return problemId_to_idx[x]\n",
    "    else: return np.nan\n",
    "    \n",
    "new_df['problemId2idx'] = new_df['problem_id'].apply(lambda x : get_problemId2idx(x))\n",
    "new_df = new_df[~(new_df['problemId2idx'].isna())].reset_index(drop = True)\n",
    "new_df['problemId2idx'] = new_df['problemId2idx'].astype(int)\n",
    "\n",
    "new_df['target2idx'] = new_df['target'].apply(lambda x : get_target2idx(x))\n",
    "new_df = new_df[~(new_df['target2idx'].isna())].reset_index(drop = True)\n",
    "new_df['target2idx'] = new_df['target2idx'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19764/19764 [00:18<00:00, 1074.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "valid_df = []\n",
    "\n",
    "group_df = new_df.groupby('target')\n",
    "for target, g_df in tqdm(group_df):\n",
    "    gg_df = g_df.groupby('key')\n",
    "    for idx, (userID, ggg_df) in enumerate(gg_df):\n",
    "        if idx == 0:\n",
    "            valid_df.append(ggg_df)\n",
    "        else:\n",
    "            train_df.append(ggg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_df).reset_index(drop = True)\n",
    "valid_df = pd.concat(valid_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'train_df.csv'), index = False)\n",
    "valid_df.to_csv(os.path.join(DATA_PATH, 'valid_df.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_assessmentItemID,\n",
    "        hidden_units,\n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        self.assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, hidden_units, padding_idx = 0) # 문항에 대한 정보\n",
    "\n",
    "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "        \n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units, num_assessmentItemID)\n",
    "        )\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        assessmentItem : (batch_size, max_len)\n",
    "        \"\"\"\n",
    "        assessmentItem = input['assessmentItem']\n",
    "\n",
    "        # masking\n",
    "        mask_pad = torch.BoolTensor(assessmentItem > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, assessmentItem.size(1), assessmentItem.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "\n",
    "        assessmentItem_emb = self.assessmentItemID_emb(assessmentItem.to(self.device))\n",
    "        for block in self.blocks:\n",
    "            assessmentItem_emb, attn_dist = block(assessmentItem_emb, mask)\n",
    "\n",
    "        assessmentItem_emb, _ = self.lstm(assessmentItem_emb)\n",
    "        \n",
    "        output = self.predict_layer(assessmentItem_emb[:, -1])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "\n",
    "        group_df = df.groupby('target')\n",
    "\n",
    "        for target2idx, g_df in group_df:\n",
    "            gg_df = g_df.groupby('key')\n",
    "            for userID, ggg_df in gg_df:\n",
    "                if len(ggg_df) >= 2:\n",
    "                    self.features.append(ggg_df['problemId2idx'].tolist()[::-1][:-1])\n",
    "                    self.targets.append(ggg_df['problemId2idx'].tolist()[::-1][-1] - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return {\n",
    "            'assessmentItem' : np.array(feature), \n",
    "            'target' : target,\n",
    "            }\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "\n",
    "def make_collate_fn(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len = sample['assessmentItem'].shape[0]\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for sample in samples:\n",
    "        features += [pad_sequence(sample['assessmentItem'], max_len = max_len, padding_value = 0)]\n",
    "        targets += [sample['target']]\n",
    "\n",
    "    return {\n",
    "        'assessmentItem' : torch.tensor(features, dtype = torch.long), \n",
    "        'target': torch.tensor(targets, dtype = torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(len(true_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit\n",
    "\n",
    "def get_rank(pred_list, true_list):\n",
    "    ret_rank = len(pred_list) + 1\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            ret_rank = rank + 1\n",
    "    return 0 if ret_rank == (len(pred_list) + 1) else ret_rank\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for input in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, input['target'].to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader, topk = 10):\n",
    "    model.eval()\n",
    "\n",
    "    hit = 0\n",
    "    ndcg = 0\n",
    "    rank = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            \n",
    "            output = -model(input)\n",
    "            predictions = output.argsort()\n",
    "\n",
    "            targets = input['target']\n",
    "\n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                target = [target.item()]\n",
    "                prediction = prediction.cpu().tolist()[:topk]\n",
    "\n",
    "                hit += get_hit(prediction, target)\n",
    "                ndcg += get_ndcg(prediction, target)\n",
    "                rank += get_rank(prediction, target)\n",
    "    \n",
    "    rank = rank / hit\n",
    "    hit = hit / len(data_loader.dataset)\n",
    "    ndcg = ndcg / len(data_loader.dataset)\n",
    "\n",
    "    return hit, ndcg, rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'problemId_to_idx.json'), 'r', encoding = 'utf-8') as f:\n",
    "    problemId_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(VAL_TO_IDX_DATA_PATH, 'idx_to_problemId.json'), 'r', encoding = 'utf-8') as f:\n",
    "    idx_to_problemId = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "hidden_units = 128\n",
    "num_heads = 8\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "num_assessmentItemID = len(problemId_to_idx)\n",
    "\n",
    "model_name = 'new-User-Seq-Transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_df.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'valid_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df = train_df)\n",
    "train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = make_collate_fn,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(df = valid_df)\n",
    "valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = make_collate_fn,\n",
    "        num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASRec(\n",
    "        num_assessmentItemID = num_assessmentItemID,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 9.69904| NDCG@10: 0.01621| HIT@10: 0.02397| Rank@10: 3.20638: 100%|██████████| 1/1 [00:40<00:00, 40.99s/it]\n",
      "Epoch:   2| Train loss: 8.65676| NDCG@10: 0.06793| HIT@10: 0.09337| Rank@10: 2.76406: 100%|██████████| 1/1 [00:39<00:00, 39.08s/it]\n",
      "Epoch:   3| Train loss: 7.49686| NDCG@10: 0.10010| HIT@10: 0.13616| Rank@10: 2.67491: 100%|██████████| 1/1 [00:35<00:00, 35.74s/it]\n",
      "Epoch:   4| Train loss: 6.62092| NDCG@10: 0.11920| HIT@10: 0.16043| Rank@10: 2.57343: 100%|██████████| 1/1 [00:39<00:00, 39.11s/it]\n",
      "Epoch:   5| Train loss: 5.92827| NDCG@10: 0.13163| HIT@10: 0.17517| Rank@10: 2.51878: 100%|██████████| 1/1 [00:38<00:00, 38.47s/it]\n",
      "Epoch:   6| Train loss: 5.35128| NDCG@10: 0.13877| HIT@10: 0.18531| Rank@10: 2.54568: 100%|██████████| 1/1 [00:35<00:00, 35.92s/it]\n",
      "Epoch:   7| Train loss: 4.85633| NDCG@10: 0.14345| HIT@10: 0.19148| Rank@10: 2.53182: 100%|██████████| 1/1 [00:37<00:00, 37.54s/it]\n",
      "Epoch:   8| Train loss: 4.42212| NDCG@10: 0.14431| HIT@10: 0.19118| Rank@10: 2.48039: 100%|██████████| 1/1 [00:39<00:00, 39.13s/it]\n",
      "Epoch:   9| Train loss: 4.03473| NDCG@10: 0.14344| HIT@10: 0.19179| Rank@10: 2.56714: 100%|██████████| 1/1 [00:38<00:00, 38.37s/it]\n",
      "Epoch:  10| Train loss: 3.69059| NDCG@10: 0.14370| HIT@10: 0.19108| Rank@10: 2.50894: 100%|██████████| 1/1 [00:37<00:00, 37.97s/it]\n",
      "Epoch:  11| Train loss: 3.37700| NDCG@10: 0.14065| HIT@10: 0.18679| Rank@10: 2.52334: 100%|██████████| 1/1 [00:38<00:00, 38.51s/it]\n",
      "Epoch:  12| Train loss: 3.09332| NDCG@10: 0.13861| HIT@10: 0.18480| Rank@10: 2.52539: 100%|██████████| 1/1 [00:36<00:00, 36.42s/it]\n",
      "Epoch:  13| Train loss: 2.83717| NDCG@10: 0.13549| HIT@10: 0.18169| Rank@10: 2.56694: 100%|██████████| 1/1 [00:36<00:00, 36.19s/it]\n",
      "Epoch:  14| Train loss: 2.60283| NDCG@10: 0.13349| HIT@10: 0.17950| Rank@10: 2.58835: 100%|██████████| 1/1 [00:35<00:00, 35.53s/it]\n",
      "Epoch:  15| Train loss: 2.38820| NDCG@10: 0.13225| HIT@10: 0.17721| Rank@10: 2.57583: 100%|██████████| 1/1 [00:36<00:00, 36.12s/it]\n",
      "Epoch:  16| Train loss: 2.18880| NDCG@10: 0.13035| HIT@10: 0.17573| Rank@10: 2.62246: 100%|██████████| 1/1 [00:36<00:00, 36.40s/it]\n",
      "Epoch:  17| Train loss: 2.00677| NDCG@10: 0.12870| HIT@10: 0.17282| Rank@10: 2.59044: 100%|██████████| 1/1 [00:37<00:00, 37.12s/it]\n",
      "Epoch:  18| Train loss: 1.84272| NDCG@10: 0.12744| HIT@10: 0.17124| Rank@10: 2.59589: 100%|██████████| 1/1 [00:37<00:00, 37.84s/it]\n",
      "Epoch:  19| Train loss: 1.69169| NDCG@10: 0.12531| HIT@10: 0.17002| Rank@10: 2.65867: 100%|██████████| 1/1 [00:36<00:00, 36.29s/it]\n",
      "Epoch:  20| Train loss: 1.54895| NDCG@10: 0.12440| HIT@10: 0.16594| Rank@10: 2.54640: 100%|██████████| 1/1 [00:36<00:00, 36.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST | Epoch:   8| Train loss: 4.42212| NDCG@10: 0.14431| HIT@10: 0.19118| Rank@10: 2.48039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_ndcg = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            hit, ndcg, rank = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_ndcg < ndcg:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_ndcg = ndcg\n",
    "                best_hit = hit\n",
    "                best_rank = rank\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, model_name + '.pt'))\n",
    "\n",
    "            tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}| Rank@10: {rank:.5f}')\n",
    "    \n",
    "print(f'BEST | Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| NDCG@10: {best_ndcg:.5f}| HIT@10: {best_hit:.5f}| Rank@10: {best_rank:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무작위 임베딩을 활용하여 seq 모델 학습\n",
    "\n",
    "```\n",
    "헌재 데이터\n",
    "Epoch:   1| Train loss: 9.69904| NDCG@10: 0.01621| HIT@10: 0.02397| Rank@10: 3.20638: 100%|██████████| 1/1 [00:40<00:00, 40.99s/it]\n",
    "Epoch:   2| Train loss: 8.65676| NDCG@10: 0.06793| HIT@10: 0.09337| Rank@10: 2.76406: 100%|██████████| 1/1 [00:39<00:00, 39.08s/it]\n",
    "Epoch:   3| Train loss: 7.49686| NDCG@10: 0.10010| HIT@10: 0.13616| Rank@10: 2.67491: 100%|██████████| 1/1 [00:35<00:00, 35.74s/it]\n",
    "Epoch:   4| Train loss: 6.62092| NDCG@10: 0.11920| HIT@10: 0.16043| Rank@10: 2.57343: 100%|██████████| 1/1 [00:39<00:00, 39.11s/it]\n",
    "Epoch:   5| Train loss: 5.92827| NDCG@10: 0.13163| HIT@10: 0.17517| Rank@10: 2.51878: 100%|██████████| 1/1 [00:38<00:00, 38.47s/it]\n",
    "Epoch:   6| Train loss: 5.35128| NDCG@10: 0.13877| HIT@10: 0.18531| Rank@10: 2.54568: 100%|██████████| 1/1 [00:35<00:00, 35.92s/it]\n",
    "Epoch:   7| Train loss: 4.85633| NDCG@10: 0.14345| HIT@10: 0.19148| Rank@10: 2.53182: 100%|██████████| 1/1 [00:37<00:00, 37.54s/it]\n",
    "Epoch:   8| Train loss: 4.42212| NDCG@10: 0.14431| HIT@10: 0.19118| Rank@10: 2.48039: 100%|██████████| 1/1 [00:39<00:00, 39.13s/it]\n",
    "Epoch:   9| Train loss: 4.03473| NDCG@10: 0.14344| HIT@10: 0.19179| Rank@10: 2.56714: 100%|██████████| 1/1 [00:38<00:00, 38.37s/it]\n",
    "Epoch:  10| Train loss: 3.69059| NDCG@10: 0.14370| HIT@10: 0.19108| Rank@10: 2.50894: 100%|██████████| 1/1 [00:37<00:00, 37.97s/it]\n",
    "Epoch:  11| Train loss: 3.37700| NDCG@10: 0.14065| HIT@10: 0.18679| Rank@10: 2.52334: 100%|██████████| 1/1 [00:38<00:00, 38.51s/it]\n",
    "Epoch:  12| Train loss: 3.09332| NDCG@10: 0.13861| HIT@10: 0.18480| Rank@10: 2.52539: 100%|██████████| 1/1 [00:36<00:00, 36.42s/it]\n",
    "Epoch:  13| Train loss: 2.83717| NDCG@10: 0.13549| HIT@10: 0.18169| Rank@10: 2.56694: 100%|██████████| 1/1 [00:36<00:00, 36.19s/it]\n",
    "Epoch:  14| Train loss: 2.60283| NDCG@10: 0.13349| HIT@10: 0.17950| Rank@10: 2.58835: 100%|██████████| 1/1 [00:35<00:00, 35.53s/it]\n",
    "Epoch:  15| Train loss: 2.38820| NDCG@10: 0.13225| HIT@10: 0.17721| Rank@10: 2.57583: 100%|██████████| 1/1 [00:36<00:00, 36.12s/it]\n",
    "Epoch:  16| Train loss: 2.18880| NDCG@10: 0.13035| HIT@10: 0.17573| Rank@10: 2.62246: 100%|██████████| 1/1 [00:36<00:00, 36.40s/it]\n",
    "Epoch:  17| Train loss: 2.00677| NDCG@10: 0.12870| HIT@10: 0.17282| Rank@10: 2.59044: 100%|██████████| 1/1 [00:37<00:00, 37.12s/it]\n",
    "Epoch:  18| Train loss: 1.84272| NDCG@10: 0.12744| HIT@10: 0.17124| Rank@10: 2.59589: 100%|██████████| 1/1 [00:37<00:00, 37.84s/it]\n",
    "Epoch:  19| Train loss: 1.69169| NDCG@10: 0.12531| HIT@10: 0.17002| Rank@10: 2.65867: 100%|██████████| 1/1 [00:36<00:00, 36.29s/it]\n",
    "Epoch:  20| Train loss: 1.54895| NDCG@10: 0.12440| HIT@10: 0.16594| Rank@10: 2.54640: 100%|██████████| 1/1 [00:36<00:00, 36.50s/it]\n",
    "BEST | Epoch:   8| Train loss: 4.42212| NDCG@10: 0.14431| HIT@10: 0.19118| Rank@10: 2.48039\n",
    "\n",
    "\n",
    "과거 데이터\n",
    "Epoch:   1| Train loss: 9.47542| NDCG@10: 0.00798| HIT@10: 0.01276| Rank@10: 3.72727: 100%|██████████| 1/1 [00:28<00:00, 28.94s/it]\n",
    "Epoch:   2| Train loss: 8.68927| NDCG@10: 0.05917| HIT@10: 0.08264| Rank@10: 2.77081: 100%|██████████| 1/1 [00:29<00:00, 29.60s/it]\n",
    "Epoch:   3| Train loss: 7.68180| NDCG@10: 0.09417| HIT@10: 0.12863| Rank@10: 2.70974: 100%|██████████| 1/1 [00:24<00:00, 24.00s/it]\n",
    "Epoch:   4| Train loss: 6.81525| NDCG@10: 0.11760| HIT@10: 0.15925| Rank@10: 2.59709: 100%|██████████| 1/1 [00:26<00:00, 26.39s/it]\n",
    "Epoch:   5| Train loss: 6.11427| NDCG@10: 0.13399| HIT@10: 0.17942| Rank@10: 2.54545: 100%|██████████| 1/1 [00:27<00:00, 27.16s/it]\n",
    "Epoch:   6| Train loss: 5.52477| NDCG@10: 0.14506| HIT@10: 0.19457| Rank@10: 2.54748: 100%|██████████| 1/1 [00:26<00:00, 26.68s/it]\n",
    "Epoch:   7| Train loss: 5.01476| NDCG@10: 0.15132| HIT@10: 0.20176| Rank@10: 2.56169: 100%|██████████| 1/1 [00:27<00:00, 27.37s/it]\n",
    "Epoch:   8| Train loss: 4.56439| NDCG@10: 0.15281| HIT@10: 0.20346| Rank@10: 2.52204: 100%|██████████| 1/1 [00:27<00:00, 27.79s/it]\n",
    "Epoch:   9| Train loss: 4.15730| NDCG@10: 0.15472| HIT@10: 0.20555| Rank@10: 2.50056: 100%|██████████| 1/1 [00:27<00:00, 27.91s/it]\n",
    "Epoch:  10| Train loss: 3.79333| NDCG@10: 0.15704| HIT@10: 0.20787| Rank@10: 2.49498: 100%|██████████| 1/1 [00:28<00:00, 28.53s/it]\n",
    "Epoch:  11| Train loss: 3.45896| NDCG@10: 0.15463| HIT@10: 0.20439| Rank@10: 2.48071: 100%|██████████| 1/1 [00:24<00:00, 24.63s/it]\n",
    "Epoch:  12| Train loss: 3.15628| NDCG@10: 0.15253| HIT@10: 0.20431| Rank@10: 2.57283: 100%|██████████| 1/1 [00:26<00:00, 26.88s/it]\n",
    "Epoch:  13| Train loss: 2.87723| NDCG@10: 0.15219| HIT@10: 0.20393| Rank@10: 2.59629: 100%|██████████| 1/1 [00:27<00:00, 27.29s/it]\n",
    "Epoch:  14| Train loss: 2.62568| NDCG@10: 0.15028| HIT@10: 0.20153| Rank@10: 2.56157: 100%|██████████| 1/1 [00:26<00:00, 26.95s/it]\n",
    "Epoch:  15| Train loss: 2.38836| NDCG@10: 0.14826| HIT@10: 0.19821| Rank@10: 2.56357: 100%|██████████| 1/1 [00:27<00:00, 27.04s/it]\n",
    "Epoch:  16| Train loss: 2.17512| NDCG@10: 0.14657| HIT@10: 0.19542| Rank@10: 2.53283: 100%|██████████| 1/1 [00:26<00:00, 26.84s/it]\n",
    "Epoch:  17| Train loss: 1.97474| NDCG@10: 0.14398| HIT@10: 0.19264| Rank@10: 2.56541: 100%|██████████| 1/1 [00:27<00:00, 27.45s/it]\n",
    "Epoch:  18| Train loss: 1.79934| NDCG@10: 0.14256| HIT@10: 0.19117| Rank@10: 2.58674: 100%|██████████| 1/1 [00:26<00:00, 26.67s/it]\n",
    "Epoch:  19| Train loss: 1.63086| NDCG@10: 0.14038| HIT@10: 0.18955| Rank@10: 2.59502: 100%|██████████| 1/1 [00:28<00:00, 28.03s/it]\n",
    "Epoch:  20| Train loss: 1.47892| NDCG@10: 0.13974| HIT@10: 0.18808| Rank@10: 2.63831: 100%|██████████| 1/1 [00:26<00:00, 26.31s/it]\n",
    "BEST | Epoch:  10| Train loss: 3.79333| NDCG@10: 0.15704| HIT@10: 0.20787| Rank@10: 2.49498\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
