{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "\n",
    "DATA_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Experiment/data'\n",
    "MODEL_PATH = '/opt/ml/final-project-level3-recsys-05/Model/Model-Experiment/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150433/150433 [02:53<00:00, 866.83it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, 'score.csv'))\n",
    "df = df[~(df['problemId'].isna())].reset_index(drop = True)\n",
    "df['problemId'] = df['problemId'].astype(int).astype(str)\n",
    "df['target'] = df['target'].astype(int).astype(str)\n",
    "df['key'] = df['username'] + '-' + df['target']\n",
    "\n",
    "new_df = []\n",
    "\n",
    "group_df = df.groupby('key')\n",
    "\n",
    "for key, g_df in tqdm(group_df):\n",
    "    new_df.append(g_df[~(g_df.duplicated('problemId'))].copy())\n",
    "\n",
    "new_df = pd.concat(new_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problemId2idx = {}\n",
    "idx2problemId = {}\n",
    "\n",
    "for idx, problemId in enumerate(new_df['problemId'].unique().tolist()):\n",
    "    problemId2idx[problemId] = idx\n",
    "    idx2problemId[idx] = problemId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['problemId2idx'] = new_df['problemId'].apply(lambda x : problemId2idx[x] + 1)\n",
    "new_df['target2idx'] = new_df['target'].apply(lambda x : problemId2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"seq-model-problemId2idx.json\", \"w\") as json_file:\n",
    "    json.dump(problemId2idx, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"seq-model-idx2problemId.json\", \"w\") as json_file:\n",
    "    json.dump(idx2problemId, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13026/13026 [00:14<00:00, 917.75it/s] \n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "valid_df = []\n",
    "\n",
    "group_df = new_df.groupby('target')\n",
    "for target, g_df in tqdm(group_df):\n",
    "    gg_df = g_df.groupby('key')\n",
    "    for idx, (userID, ggg_df) in enumerate(gg_df):\n",
    "        if idx == 0:\n",
    "            valid_df.append(ggg_df)\n",
    "        else:\n",
    "            train_df.append(ggg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_df).reset_index(drop = True)\n",
    "valid_df = pd.concat(valid_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'train_df.csv'))\n",
    "valid_df.to_csv(os.path.join(DATA_PATH, 'valid_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        \"\"\"\n",
    "        enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units // self.num_heads) # (batch_size, max_len, num_heads, hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
    "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        \"\"\"\n",
    "        input_enc : (batch_size, max_len, hidden_units)\n",
    "        mask : (batch_size, 1, max_len, max_len)\n",
    "        \"\"\"\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_assessmentItemID,\n",
    "        hidden_units,\n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        dropout_rate):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        self.assessmentItemID_emb = nn.Embedding(num_assessmentItemID + 1, hidden_units, padding_idx = 0) # 문항에 대한 정보\n",
    "\n",
    "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = hidden_units,\n",
    "            hidden_size = hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_rate,\n",
    "            )\n",
    "        \n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_units, num_assessmentItemID)\n",
    "        )\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        assessmentItem : (batch_size, max_len)\n",
    "        \"\"\"\n",
    "        assessmentItem = input['assessmentItem']\n",
    "\n",
    "        # masking\n",
    "        mask_pad = torch.BoolTensor(assessmentItem > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1, 1, assessmentItem.size(1), assessmentItem.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
    "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
    "\n",
    "        assessmentItem_emb = self.assessmentItemID_emb(assessmentItem.to(self.device))\n",
    "        for block in self.blocks:\n",
    "            assessmentItem_emb, attn_dist = block(assessmentItem_emb, mask)\n",
    "\n",
    "        assessmentItem_emb, _ = self.lstm(assessmentItem_emb)\n",
    "        \n",
    "        output = self.predict_layer(assessmentItem_emb[:, -1])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "\n",
    "        group_df = df.groupby('target')\n",
    "\n",
    "        for target2idx, g_df in group_df:\n",
    "            gg_df = g_df.groupby('key')\n",
    "            for userID, ggg_df in gg_df:\n",
    "                if len(ggg_df) >= 2:\n",
    "                    self.features.append(ggg_df['problemId2idx'].tolist()[::-1][:-1])\n",
    "                    self.targets.append(ggg_df['problemId2idx'].tolist()[::-1][-1] - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return {\n",
    "            'assessmentItem' : np.array(feature), \n",
    "            'target' : target,\n",
    "            }\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "\n",
    "def make_collate_fn(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len = sample['assessmentItem'].shape[0]\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for sample in samples:\n",
    "        features += [pad_sequence(sample['assessmentItem'], max_len = max_len, padding_value = 0)]\n",
    "        targets += [sample['target']]\n",
    "\n",
    "    return {\n",
    "        'assessmentItem' : torch.tensor(features, dtype = torch.long), \n",
    "        'target': torch.tensor(targets, dtype = torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(len(true_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit\n",
    "\n",
    "def get_rank(pred_list, true_list):\n",
    "    ret_rank = len(pred_list)\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            ret_rank = rank + 1\n",
    "    return ret_rank\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for input in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, input['target'].to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader, topk = 10):\n",
    "    model.eval()\n",
    "\n",
    "    hit = 0\n",
    "    ndcg = 0\n",
    "    rank = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            \n",
    "            output = -model(input)\n",
    "            predictions = output.argsort()\n",
    "\n",
    "            targets = input['target']\n",
    "\n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                target = [target.item()]\n",
    "                prediction = prediction.cpu().tolist()[:topk]\n",
    "\n",
    "                hit += get_hit(prediction, target)\n",
    "                ndcg += get_ndcg(prediction, target)\n",
    "                rank += get_rank(prediction, target)\n",
    "\n",
    "    hit, ndcg, rank = hit / len(data_loader.dataset), ndcg / len(data_loader.dataset), rank / len(data_loader.dataset)\n",
    "\n",
    "    return hit, ndcg, rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_df.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'valid_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "hidden_units = 128\n",
    "num_heads = 8\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "\n",
    "model_name = 'User-Seq-Transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df = train_df)\n",
    "train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = make_collate_fn,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(df = valid_df)\n",
    "valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = make_collate_fn,\n",
    "        num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASRec(\n",
    "        num_assessmentItemID = 19156,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 9.42720| NDCG@10: 0.00664| HIT@10: 0.01082| Rank@10: 9.93646: 100%|██████████| 1/1 [00:23<00:00, 23.27s/it]\n",
      "Epoch:   2| Train loss: 8.61495| NDCG@10: 0.06213| HIT@10: 0.08627| Rank@10: 9.38227: 100%|██████████| 1/1 [00:22<00:00, 22.24s/it]\n",
      "Epoch:   3| Train loss: 7.57542| NDCG@10: 0.09878| HIT@10: 0.13428| Rank@10: 9.01894: 100%|██████████| 1/1 [00:22<00:00, 22.07s/it]\n",
      "Epoch:   4| Train loss: 6.70863| NDCG@10: 0.12159| HIT@10: 0.16450| Rank@10: 8.79035: 100%|██████████| 1/1 [00:22<00:00, 22.71s/it]\n",
      "Epoch:   5| Train loss: 6.01424| NDCG@10: 0.13400| HIT@10: 0.17996| Rank@10: 8.65724: 100%|██████████| 1/1 [00:22<00:00, 22.41s/it]\n",
      "Epoch:   6| Train loss: 5.43340| NDCG@10: 0.14418| HIT@10: 0.19194| Rank@10: 8.55373: 100%|██████████| 1/1 [00:20<00:00, 20.40s/it]\n",
      "Epoch:   7| Train loss: 4.93029| NDCG@10: 0.15001| HIT@10: 0.19952| Rank@10: 8.49760: 100%|██████████| 1/1 [00:20<00:00, 20.17s/it]\n",
      "Epoch:   8| Train loss: 4.48561| NDCG@10: 0.15299| HIT@10: 0.20524| Rank@10: 8.47395: 100%|██████████| 1/1 [00:20<00:00, 20.33s/it]\n",
      "Epoch:   9| Train loss: 4.08519| NDCG@10: 0.15464| HIT@10: 0.20756| Rank@10: 8.45756: 100%|██████████| 1/1 [00:20<00:00, 20.55s/it]\n",
      "Epoch:  10| Train loss: 3.72310| NDCG@10: 0.15456| HIT@10: 0.20741| Rank@10: 8.45393: 100%|██████████| 1/1 [00:20<00:00, 20.63s/it]\n",
      "Epoch:  11| Train loss: 3.39486| NDCG@10: 0.15402| HIT@10: 0.20625| Rank@10: 8.46081: 100%|██████████| 1/1 [00:21<00:00, 21.23s/it]\n",
      "Epoch:  12| Train loss: 3.09600| NDCG@10: 0.15221| HIT@10: 0.20501| Rank@10: 8.48052: 100%|██████████| 1/1 [00:23<00:00, 23.26s/it]\n",
      "Epoch:  13| Train loss: 2.81946| NDCG@10: 0.15054| HIT@10: 0.20130| Rank@10: 8.49598: 100%|██████████| 1/1 [00:22<00:00, 22.69s/it]\n",
      "Epoch:  14| Train loss: 2.56883| NDCG@10: 0.14960| HIT@10: 0.20145| Rank@10: 8.51136: 100%|██████████| 1/1 [00:20<00:00, 20.61s/it]\n",
      "Epoch:  15| Train loss: 2.33966| NDCG@10: 0.14703| HIT@10: 0.19736| Rank@10: 8.53973: 100%|██████████| 1/1 [00:20<00:00, 20.09s/it]\n",
      "Epoch:  16| Train loss: 2.12856| NDCG@10: 0.14548| HIT@10: 0.19589| Rank@10: 8.55087: 100%|██████████| 1/1 [00:20<00:00, 20.72s/it]\n",
      "Epoch:  17| Train loss: 1.93166| NDCG@10: 0.14304| HIT@10: 0.19365| Rank@10: 8.58071: 100%|██████████| 1/1 [00:22<00:00, 22.13s/it]\n",
      "Epoch:  18| Train loss: 1.75575| NDCG@10: 0.14284| HIT@10: 0.19164| Rank@10: 8.57699: 100%|██████████| 1/1 [00:20<00:00, 20.94s/it]\n",
      "Epoch:  19| Train loss: 1.59032| NDCG@10: 0.13933| HIT@10: 0.18707| Rank@10: 8.61619: 100%|██████████| 1/1 [00:20<00:00, 20.90s/it]\n",
      "Epoch:  20| Train loss: 1.44164| NDCG@10: 0.13873| HIT@10: 0.18777| Rank@10: 8.62423: 100%|██████████| 1/1 [00:20<00:00, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST | Epoch:   9| Train loss: 4.08519| NDCG@10: 0.15464| HIT@10: 0.20756| Rank@10: 8.45756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_ndcg = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            hit, ndcg, rank = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_ndcg < ndcg:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_ndcg = ndcg\n",
    "                best_hit = hit\n",
    "                best_rank = rank\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, model_name + '.pt'))\n",
    "\n",
    "            tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}| Rank@10: {rank:.5f}')\n",
    "    \n",
    "print(f'BEST | Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| NDCG@10: {best_ndcg:.5f}| HIT@10: {best_hit:.5f}| Rank@10: {best_rank:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실 본 데이터는 이미 seq 안에 문제집 순서에 대한 정보가 반영되어 있기 때문에 hit가 높은 것은 당연함\n",
    "\n",
    "본 결과의 시사점은 단순히 seq를 바탕으로 다음 타겟을 예측하더라도 어느 정도의 정확도는 나온다는 것에 있음\n",
    "\n",
    "따라서 이제 백준 seq를 바탕으로 다음 target에 대한 예측을 진행한 후 어떤 문제들을 추천해주는지 알아보면 될 듯 함"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
